{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    },
    "colab": {
      "name": "roberta_for_argument_classification.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6guYEySJYh6"
      },
      "source": [
        "# Using RoBERTa with Fastai Tutorial "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3d1unLxrJYh-"
      },
      "source": [
        "This notebook follows the tutorial @ https://medium.com/@devkosal/using-roberta-with-fastai-for-nlp-7ed3fed21f6c"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m1WviXrTJ2lj",
        "outputId": "49bdbb88-51eb-40a0-a2d0-648bc48a3f73"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ka1j15OzJ9C2",
        "outputId": "c6fa7135-3594-496d-e09c-03f65cc20a7c"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.6.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (4.5.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n",
            "Requirement already satisfied: huggingface-hub==0.0.8 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.8)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vUihnFVOJ-_4",
        "outputId": "bae305de-facb-4099-ac16-79e22dbcebbe"
      },
      "source": [
        "!pip install fastai"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: fastai in /usr/local/lib/python3.7/dist-packages (1.0.61)\n",
            "Requirement already satisfied: nvidia-ml-py3 in /usr/local/lib/python3.7/dist-packages (from fastai) (7.352.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from fastai) (4.6.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from fastai) (7.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from fastai) (2.23.0)\n",
            "Requirement already satisfied: fastprogress>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from fastai) (1.0.0)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.7/dist-packages (from fastai) (2.7.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from fastai) (20.9)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.7/dist-packages (from fastai) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from fastai) (1.4.1)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from fastai) (1.8.1+cu101)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from fastai) (0.9.1+cu101)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from fastai) (3.13)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from fastai) (3.2.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from fastai) (1.1.5)\n",
            "Requirement already satisfied: spacy>=2.0.18; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from fastai) (2.2.4)\n",
            "Requirement already satisfied: bottleneck in /usr/local/lib/python3.7/dist-packages (from fastai) (1.3.2)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->fastai) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->fastai) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->fastai) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->fastai) (2.10)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->fastai) (2.4.7)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.0.0->fastai) (3.7.4.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->fastai) (1.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->fastai) (2.8.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->fastai) (0.10.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->fastai) (2018.9)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.18; python_version < \"3.8\"->fastai) (0.4.1)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.18; python_version < \"3.8\"->fastai) (1.0.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.18; python_version < \"3.8\"->fastai) (1.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.18; python_version < \"3.8\"->fastai) (57.0.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.18; python_version < \"3.8\"->fastai) (0.8.2)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.18; python_version < \"3.8\"->fastai) (7.4.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.18; python_version < \"3.8\"->fastai) (1.0.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.18; python_version < \"3.8\"->fastai) (4.41.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.18; python_version < \"3.8\"->fastai) (2.0.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.18; python_version < \"3.8\"->fastai) (3.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.18; python_version < \"3.8\"->fastai) (1.0.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->fastai) (1.15.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.0.18; python_version < \"3.8\"->fastai) (4.5.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.0.18; python_version < \"3.8\"->fastai) (3.4.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-09-04T13:50:53.112411Z",
          "start_time": "2019-09-04T13:50:52.024302Z"
        },
        "id": "--5eQkOFJYiB"
      },
      "source": [
        "from fastai.text import *\n",
        "from fastai.metrics import *\n",
        "from transformers import RobertaTokenizer"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-09-04T13:50:53.119474Z",
          "start_time": "2019-09-04T13:50:53.113824Z"
        },
        "id": "fs3BbefPJYiD"
      },
      "source": [
        "# Creating a config object to store task specific information\n",
        "class Config(dict):\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        for k, v in kwargs.items():\n",
        "            setattr(self, k, v)\n",
        "    \n",
        "    def set(self, key, val):\n",
        "        self[key] = val\n",
        "        setattr(self, key, val)\n",
        "        \n",
        "config = Config(\n",
        "    testing=False,\n",
        "    seed = 2019,\n",
        "    roberta_model_name='roberta-base', # can also be exchnaged with roberta-large \n",
        "    max_lr=1e-5,\n",
        "    epochs=1,\n",
        "    use_fp16=False,\n",
        "    bs=4, \n",
        "    max_seq_len=256, \n",
        "    num_labels = 4,\n",
        "    hidden_dropout_prob=.05,\n",
        "    hidden_size=768, # 1024 for roberta-large\n",
        "    start_tok = \"<s>\",\n",
        "    end_tok = \"</s>\",\n",
        ")"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-09-04T13:50:53.557101Z",
          "start_time": "2019-09-04T13:50:53.122045Z"
        },
        "id": "n-m5NvrZJYiE"
      },
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/app_reviews_all_annotated2.csv')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-09-04T13:50:53.562452Z",
          "start_time": "2019-09-04T13:50:53.558919Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s5jXv3QhJYiF",
        "outputId": "93ac8f9b-3933-4d84-d45e-327f2ab7f46d"
      },
      "source": [
        "if config.testing: df = df[:5000]\n",
        "print(df.shape)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(46103, 8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-09-04T13:50:53.583364Z",
          "start_time": "2019-09-04T13:50:53.564068Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "FolDSELOJYiH",
        "outputId": "6d9afe31-946b-4171-a450-768f1c2416a5"
      },
      "source": [
        "df = df[['review', 'argument_cat']]\n",
        "# Remove missing rows\n",
        "df = df.dropna()\n",
        "df"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>argument_cat</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>owsm   Full Review</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Amazing  app for edit   Full Review</td>\n",
              "      <td>Arg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Thank you SO much for ALL of the pointers!! ...</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>It's awesome   Full Review</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Great   Full Review</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46098</th>\n",
              "      <td>Cool   Full Review</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46099</th>\n",
              "      <td>Good   Full Review</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46100</th>\n",
              "      <td>Best   Full Review</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46101</th>\n",
              "      <td>Nice game   Full Review</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46102</th>\n",
              "      <td>Awesome   Full Review</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>46090 rows Ã— 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  review argument_cat\n",
              "0                                  owsm   Full Review            None\n",
              "1                 Amazing  app for edit   Full Review             Arg\n",
              "2        Thank you SO much for ALL of the pointers!! ...         None\n",
              "3                          It's awesome   Full Review            None\n",
              "4                                 Great   Full Review            None\n",
              "...                                                  ...          ...\n",
              "46098                              Cool   Full Review            None\n",
              "46099                              Good   Full Review            None\n",
              "46100                              Best   Full Review            None\n",
              "46101                         Nice game   Full Review            None\n",
              "46102                           Awesome   Full Review            None\n",
              "\n",
              "[46090 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-09-04T13:50:53.591663Z",
          "start_time": "2019-09-04T13:50:53.585091Z"
        },
        "id": "fY3B9gIPJYiI"
      },
      "source": [
        "feat_cols = \"review\"\n",
        "label_cols = \"argument_cat\""
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5luRAqM6JYiI"
      },
      "source": [
        "## Setting Up the Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-09-04T13:50:53.665067Z",
          "start_time": "2019-09-04T13:50:53.657647Z"
        },
        "id": "YoObWdU9JYiJ"
      },
      "source": [
        "class FastAiRobertaTokenizer(BaseTokenizer):\n",
        "    \"\"\"Wrapper around RobertaTokenizer to be compatible with fastai\"\"\"\n",
        "    def __init__(self, tokenizer: RobertaTokenizer, max_seq_len: int=128, **kwargs): \n",
        "        self._pretrained_tokenizer = tokenizer\n",
        "        self.max_seq_len = max_seq_len \n",
        "    def __call__(self, *args, **kwargs): \n",
        "        return self \n",
        "    def tokenizer(self, t:str) -> List[str]: \n",
        "        \"\"\"Adds Roberta bos and eos tokens and limits the maximum sequence length\"\"\" \n",
        "        return [config.start_tok] + self._pretrained_tokenizer.tokenize(t)[:self.max_seq_len - 2] + [config.end_tok]"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-09-04T13:50:54.632414Z",
          "start_time": "2019-09-04T13:50:54.275831Z"
        },
        "id": "9HlXaucrJYiK"
      },
      "source": [
        "# create fastai tokenizer for roberta\n",
        "roberta_tok = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
        "\n",
        "fastai_tokenizer = Tokenizer(tok_func=FastAiRobertaTokenizer(roberta_tok, max_seq_len=config.max_seq_len), \n",
        "                             pre_rules=[], post_rules=[])"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-09-04T13:50:55.213017Z",
          "start_time": "2019-09-04T13:50:55.095284Z"
        },
        "id": "Avr2TSPQJYiL"
      },
      "source": [
        "# create fastai vocabulary for roberta\n",
        "path = Path()\n",
        "roberta_tok.save_vocabulary(path)\n",
        "\n",
        "with open('vocab.json', 'r') as f:\n",
        "    roberta_vocab_dict = json.load(f)\n",
        "    \n",
        "fastai_roberta_vocab = Vocab(list(roberta_vocab_dict.keys()))"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-09-04T13:50:55.920906Z",
          "start_time": "2019-09-04T13:50:55.914824Z"
        },
        "id": "VrJHtvYXJYiL"
      },
      "source": [
        "# Setting up pre-processors\n",
        "class RobertaTokenizeProcessor(TokenizeProcessor):\n",
        "    def __init__(self, tokenizer):\n",
        "         super().__init__(tokenizer=tokenizer, include_bos=False, include_eos=False)\n",
        "\n",
        "class RobertaNumericalizeProcessor(NumericalizeProcessor):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "\n",
        "def get_roberta_processor(tokenizer:Tokenizer=None, vocab:Vocab=None):\n",
        "    \"\"\"\n",
        "    Constructing preprocessors for Roberta\n",
        "    We remove sos and eos tokens since we add that ourselves in the tokenizer.\n",
        "    We also use a custom vocabulary to match the numericalization with the original Roberta model.\n",
        "    \"\"\"\n",
        "    return [RobertaTokenizeProcessor(tokenizer=tokenizer), RobertaNumericalizeProcessor(vocab=vocab)]"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_Z4JxO8JYiM"
      },
      "source": [
        "## Setting up the DataBunch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-09-04T13:50:57.966052Z",
          "start_time": "2019-09-04T13:50:57.959106Z"
        },
        "id": "dHWtX46QJYiM"
      },
      "source": [
        "# Creating a Roberta specific DataBunch class\n",
        "class RobertaDataBunch(TextDataBunch):\n",
        "    \"Create a `TextDataBunch` suitable for training Roberta\"\n",
        "    @classmethod\n",
        "    def create(cls, train_ds, valid_ds, test_ds=None, path:PathOrStr='.', bs:int=64, val_bs:int=None, pad_idx=1,\n",
        "               pad_first=True, device:torch.device=None, no_check:bool=False, backwards:bool=False, \n",
        "               dl_tfms:Optional[Collection[Callable]]=None, **dl_kwargs) -> DataBunch:\n",
        "        \"Function that transform the `datasets` in a `DataBunch` for classification. Passes `**dl_kwargs` on to `DataLoader()`\"\n",
        "        datasets = cls._init_ds(train_ds, valid_ds, test_ds)\n",
        "        val_bs = ifnone(val_bs, bs)\n",
        "        collate_fn = partial(pad_collate, pad_idx=pad_idx, pad_first=pad_first, backwards=backwards)\n",
        "        train_sampler = SortishSampler(datasets[0].x, key=lambda t: len(datasets[0][t][0].data), bs=bs)\n",
        "        train_dl = DataLoader(datasets[0], batch_size=bs, sampler=train_sampler, drop_last=True, **dl_kwargs)\n",
        "        dataloaders = [train_dl]\n",
        "        for ds in datasets[1:]:\n",
        "            lengths = [len(t) for t in ds.x.items]\n",
        "            sampler = SortSampler(ds.x, key=lengths.__getitem__)\n",
        "            dataloaders.append(DataLoader(ds, batch_size=val_bs, sampler=sampler, **dl_kwargs))\n",
        "        return cls(*dataloaders, path=path, device=device, dl_tfms=dl_tfms, collate_fn=collate_fn, no_check=no_check)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-09-04T13:50:58.992534Z",
          "start_time": "2019-09-04T13:50:58.986024Z"
        },
        "id": "WEkXxmURJYiM"
      },
      "source": [
        "class RobertaTextList(TextList):\n",
        "    _bunch = RobertaDataBunch\n",
        "    _label_cls = TextList"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-09-04T13:51:22.079406Z",
          "start_time": "2019-09-04T13:51:00.215358Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "oSblHcE9JYiN",
        "outputId": "585c637e-0c18-459a-9713-aa1d63bf996e"
      },
      "source": [
        "# loading the tokenizer and vocab processors\n",
        "processor = get_roberta_processor(tokenizer=fastai_tokenizer, vocab=fastai_roberta_vocab)\n",
        "\n",
        "# creating our databunch \n",
        "data = RobertaTextList.from_df(df, \".\", cols=feat_cols, processor=processor) \\\n",
        "    .split_by_rand_pct(seed=config.seed) \\\n",
        "    .label_from_df(cols=label_cols,label_cls=CategoryList) \\\n",
        "    .databunch(bs=config.bs, pad_first=False, pad_idx=0)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/fastai/core.py:302: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  return np.array(a, dtype=dtype, **kwargs)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X18HH53TJYiO"
      },
      "source": [
        "# Building the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-09-04T13:54:26.664096Z",
          "start_time": "2019-09-04T13:54:26.657203Z"
        },
        "id": "J9PKVSdXJYiO"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import RobertaModel\n",
        "\n",
        "# defining our model architecture \n",
        "class CustomRobertaModel(nn.Module):\n",
        "    def __init__(self,num_labels=2):\n",
        "        super(CustomRobertaModel,self).__init__()\n",
        "        self.num_labels = num_labels\n",
        "        self.roberta = RobertaModel.from_pretrained(config.roberta_model_name)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        self.classifier = nn.Linear(config.hidden_size, num_labels) # defining final output layer\n",
        "        \n",
        "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None):\n",
        "        _ , pooled_output = self.roberta(input_ids, token_type_ids, attention_mask,return_dict=False) # \n",
        "        logits = self.classifier(pooled_output)        \n",
        "        return logits"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-09-04T13:54:37.375403Z",
          "start_time": "2019-09-04T13:54:34.031342Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ws4SfJvJYiO",
        "outputId": "f1fc4759-4dca-45eb-d415-d195bd22b263"
      },
      "source": [
        "roberta_model = CustomRobertaModel(num_labels=config.num_labels)\n",
        "\n",
        "learn = Learner(data, roberta_model, metrics=[accuracy])"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-09-02T14:20:31.006732Z",
          "start_time": "2019-09-02T14:14:45.072892Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "id": "A8gerlfLJYiO",
        "outputId": "384144ed-e4e6-4881-b55f-7e8f8839d24e"
      },
      "source": [
        "learn.model.roberta.train() # setting roberta to train as it is in eval mode by default\n",
        "learn.fit_one_cycle(config.epochs, max_lr=config.max_lr)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.307298</td>\n",
              "      <td>0.290057</td>\n",
              "      <td>0.885116</td>\n",
              "      <td>25:40</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1bss5ljJYiP"
      },
      "source": [
        "# Getting Predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-09-02T14:11:19.976614Z",
          "start_time": "2019-09-02T14:11:19.970844Z"
        },
        "id": "StOekhpnJYiP"
      },
      "source": [
        "def get_preds_as_nparray(ds_type) -> np.ndarray:\n",
        "    learn.model.roberta.eval()\n",
        "    preds = learn.get_preds(ds_type)[0].detach().cpu().numpy()\n",
        "    sampler = [i for i in data.dl(ds_type).sampler]\n",
        "    reverse_sampler = np.argsort(sampler)\n",
        "    ordered_preds = preds[reverse_sampler, :]\n",
        "    pred_values = np.argmax(ordered_preds, axis=1)\n",
        "    return ordered_preds, pred_values"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-09-02T14:11:40.835200Z",
          "start_time": "2019-09-02T14:11:20.102674Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "qtxX-oZmJYiP",
        "outputId": "8d774cf0-5f82-4d5d-a92a-60497f54165a"
      },
      "source": [
        "preds, pred_values = get_preds_as_nparray(DatasetType.Valid)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-09-02T14:11:40.840225Z",
          "start_time": "2019-09-02T14:11:40.837056Z"
        },
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hS691AQeJYiR",
        "outputId": "a306cef5-91fb-4d9a-debf-a146f1a97a4a"
      },
      "source": [
        "# accuracy on valid\n",
        "(pred_values == data.valid_ds.y.items).mean()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8851160772401823"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tNesXCEZVK-s",
        "outputId": "c33ee57f-3ab6-46ff-b11a-cdb59c199d3d"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "y_test = data.valid_ds.y.items\n",
        "y_pred = pred_values\n",
        "\n",
        "print('Confusion matrix:')\n",
        "print(confusion_matrix(y_test,y_pred,labels=[0,1,2,3]))\n",
        "print()\n",
        "print('Classification report:')\n",
        "print(classification_report(y_test,y_pred,labels=[0,1,2,3],target_names=['None','Arg','Dec','Both']))"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Confusion matrix:\n",
            "[[3030  102    0  273]\n",
            " [ 174  258    6   13]\n",
            " [  18   18   25   30]\n",
            " [ 402   10   13 4846]]\n",
            "\n",
            "Classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        None       0.84      0.89      0.86      3405\n",
            "         Arg       0.66      0.57      0.62       451\n",
            "         Dec       0.57      0.27      0.37        91\n",
            "        Both       0.94      0.92      0.93      5271\n",
            "\n",
            "    accuracy                           0.89      9218\n",
            "   macro avg       0.75      0.66      0.69      9218\n",
            "weighted avg       0.88      0.89      0.88      9218\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OO-mBpPCVpU8",
        "outputId": "7a1d1ed8-2b27-4d8f-e8fe-83aea011bf8b"
      },
      "source": [
        "np.unique(y_pred)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 2, 3])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7vwPNUJJYiR"
      },
      "source": [
        "# Optional: Saving/Loading the model weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUEk57PfJYiR"
      },
      "source": [
        "Once you are satisfied with training, you can use the following methods to save and load your pretrained weights."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tm0KLY-DJYiR"
      },
      "source": [
        "def save_model(learner, file_name):\n",
        "    st = learner.model.state_dict()\n",
        "    torch.save(st, file_name) # will save model in current dir # backend is pickle \n",
        "\n",
        "def load_model(learner, file_name):\n",
        "    st = torch.load(file_name)\n",
        "    learner.model.load_state_dict(st)\n",
        "\n",
        "# monkey patching Learner methods to save and load model file\n",
        "Learner.save_model = save_model\n",
        "Learner.load_model = load_model"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOQmUW4IJYiR"
      },
      "source": [
        "# learn.save_model(\"my_model.pth\")\n",
        "\n",
        "# learn.load_model(\"my_model.pth\")"
      ],
      "execution_count": 24,
      "outputs": []
    }
  ]
}